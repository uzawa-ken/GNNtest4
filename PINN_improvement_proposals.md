# PINN改善アイデアの追加メモ

## 微分演算子の直接損失追加（オートディファレンスでラプラシアン評価）
- PDE行列ベクトル積ベースの残差に加え、モデル出力 `x_pred` を座標で自動微分してラプラシアンや流束ダイバージェンスを直接計算し、`||\nabla^2 x_pred||` や `||\nabla\cdot F(x_pred)||` を損失に加える。
- ケース依存の差分式に縛られず、方程式変更時も微分演算子を差し替えるだけで損失を組み立て可能になる。

## 適応的コロケーション戦略（残差重視サンプリング）
- エポック毎にPDE残差ヒートマップを集計し、残差が上位q分位にあるセル/コロケーション点を優先的に再サンプリングする。
- 低残差領域はサンプリング密度を下げ、計算量を抑えつつ難所を重点学習する適応メッシュリファインメント風の挙動をPINNで模倣する。

## 境界条件損失の明示化（WALL_FACES活用）
- `WALL_FACES` など境界メタデータから境界ノード・法線を抽出し、Dirichletなら `(x_pred - x_bc)^2`、Neumannなら `(\nabla x_pred \cdot n - g)^2` をマスク付きで追加する。
- PDE行列への折り込みに依存せず、境界条件遵守度をログ・検証指標としても記録可能にする。

## モデルアーキテクチャ拡張
- **Fourier特徴量**: 入力座標に高周波サイン・コサイン埋め込みを付与し、長距離依存や高周波解を表現しやすくする。
- **残差接続**: GNN/MLPブロック間にスキップ接続を挿入し、深さを増やしても勾配消失を緩和。
- **マルチスケール出力**: 粗～細スケールのヘッドを複数用意し、PDE/境界損失をスケール別に課すことで局所・大域特性を同時に学習。

## カリキュラム学習戦略（データ→PDEへの段階移行）
- 初期エポックは `USE_DATA_LOSS=True` で教師信号を強め、徐々に `LAMBDA_DATA` を減衰させ `LAMBDA_PDE` を増加。
- 後半でデータ損失を0にしPDE＋境界＋ゲージのみで仕上げると、完全PINN移行が滑らかになり発散を防ぎやすい。

## 残差ベースの注意機構
- PDE残差をノード重みとしてメッセージパッシングに与え、残差の大きい領域へ注意を集中させる。
- 残差を入力特徴として追加するか、ゲーティング係数としてエッジ/ノード更新に乗せる実装が考えられる。

## 損失関数再設計（滑らかさ・時間整合性）
- 空間滑らかさ: 勾配ノルムやTV正則化 `||\nabla x_pred||` を抑制する項で非物理解を抑える。
- 時間整合性: 時系列データがある場合は `||x_{t+1} - \Phi(x_t)||` など時間更新の物理モデルをソフト拘束として入れる。
- 保存則: 質量/エネルギーなどの積分値を計算し、目標値との偏差を損失に追加して物理整合性を強化。
